{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "020c7038-89ef-411b-9ee5-1fee71dd31d6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "use strange;\n",
    "CREATE TABLE person (\n",
    "    id STRING,\n",
    "    name STRING,\n",
    "    ADDR STRING,\n",
    "    DATE STRING,\n",
    "    Active BOOLEAN    \n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "37587276-5811-417a-99ba-f75bba762735",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, lit, when\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "spark = SparkSession.builder.appName(\"SCD2-Incremental\").getOrCreate()\n",
    "\n",
    "# Existing SCD2 Table\n",
    "existing_data = [\n",
    "    (\"1\",\"A\",\"Pune\",\"2025-07-01\", True),\n",
    "    (\"2\",\"B\",\"Pune\",\"2025-07-15\", True),   \n",
    "    (\"3\",\"C\",\"Pune\",\"2025-07-15\", True),# currently active\n",
    "]\n",
    "columns = [\"ID\",\"NAME\",\"ADDR\",\"DATE\",\"Active\"]\n",
    "df_existing = spark.createDataFrame(existing_data, columns)\n",
    "\n",
    "print(\"Current SCD2 Table:\")\n",
    "df_existing.createOrReplaceTempView(\"person_data\")\n",
    "df_existing.show()\n",
    "\n",
    "# New incoming data\n",
    "new_data = [(\"4\",\"D\",\"Goa\",\"2025-07-25\"),\n",
    "            (\"5\",\"E\",\"Pune\",\"2025-08-25\"),\n",
    "            (\"3\",\"C\",\"Mumbai\",\"2025-08-25\"),\n",
    "            (\"2\",\"B\",\"Pune\",\"2025-07-15\"),\n",
    "            ]\n",
    "columns_new = [\"ID\",\"NAME\",\"ADDR\",\"DATE\"]\n",
    "df_new = spark.createDataFrame(new_data, columns_new)\n",
    "\n",
    "print(\"New Data:\")\n",
    "df_new.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d920a36b-46dd-4429-837f-28cd53562030",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "--insert into person  \n",
    "--select * from person_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d946a264-1bf7-42b4-9e6f-436b0782b58a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "use strange;\n",
    "select * from person"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "042da3f5-78e7-444d-8daa-a2beffa31101",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 1.From Target table remove audit columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e062a5d9-e4d5-467e-bd0b-33a126a0ff85",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_person = spark.read.table(\"strange.person\")\n",
    "df_target  = df_person.select(\"ID\",\"NAME\",\"ADDR\",\"DATE\") \n",
    "df_target.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6fad696f-3e27-4b95-b461-cc54e87d9e39",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 2.use xxhash64 compare source and target df excluding audit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "38a0dbe9-7c68-412e-b460-5d983cc290e8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import xxhash64\n",
    "\n",
    "#df_target= df_target.withColumn(\"hash64\",xxhash64(col(\"ID\"),col(\"NAME\"),col(\"ADDR\"),col(\"DATE\")))\n",
    "df_target= df_target.withColumn(\"hash64\",xxhash64(\"ID\",\"NAME\",\"ADDR\",\"DATE\"))\n",
    "#df_new= df_new.withColumn(\"hash64\",xxhash64(col(\"ID\"),col(\"NAME\"),col(\"ADDR\"),col(\"DATE\")))\n",
    "\n",
    "df_target.show()\n",
    "#df_new.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9f3f6dee-c701-49f6-9a3d-0c3268eb0314",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###  3. perform left anti join to remove same records which are not changed exmaple id 2 no change\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a3be8f4-35ba-49f3-a041-1afdb7239e28",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_result = df_new.join(df_target,\"hash64\",\"left_anti\").drop(\"hash64\")\n",
    "df_result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "84ff96c8-6316-4702-a6e3-14133b69d371",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 4.Add audit columns to result df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f47c8730-5e51-40af-a6de-11e31d642b50",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import current_timestamp,to_timestamp\n",
    "\n",
    "df_result = df_result.withColumn(\"Active\",lit(True))\n",
    " # .withColumn(\"start_time\",to_timestamp(current_timestamp(),\"yyyy-MM-dd HH:mm:ss\"))\\\n",
    " #   .withColumn(\"End_time\", to_timestamp(lit(\"9999-12-31 00:00:00\"),\"yyyy-MM-dd HH:mm:ss\"))\n",
    "\n",
    "df_result.createOrReplaceTempView(\"person_result\")\n",
    "df_result.show(truncate = False )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "04c2a4c9-cdd0-4cde-8bb2-fa39ef2d03cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 5. Write result into target table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ade22066-31c4-420f-9eda-4ffa3a664a2f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "insert into person  \n",
    "select * from person_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "164a364c-bc68-43e8-8058-55cead3050eb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Generate Row number partition by id order by date desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a205a87a-0e9f-4063-afc8-e4e9e2f79955",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "df_final = spark.sql('''\n",
    "with cte as (\n",
    "select id,name,addr,date, row_number() over(partition by id order by date desc) as rnk,\n",
    "(case when  rnk >1 then False else Active end) as Active\n",
    "from person\n",
    ")\n",
    "select id,name,addr,date,active from cte \n",
    "''')\n",
    "df_final.show(truncate = False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5554423a-9df3-4b94-9cf4-ce8f96eabafc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### for updating start date and end date of old records us lag "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "677bd2ad-131e-4735-8e00-cdb006fc9725",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "windspec = Window.partitionBy(\"id\").orderBy(\"date\").desc()\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 4921366519739709,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "SCD Type2",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
