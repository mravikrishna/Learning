{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "949024e9-a1d4-410a-831d-aa8dbd8b9bc9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col,trim\n",
    "data = [\n",
    "    (\"B\", \"Pune\",   100, \"2023-01-01\"),\n",
    "    (\"R\", None,     None, \"2023-01-02\"),\n",
    "    (\"M\", \"Delhi\",  200, \"2023-01-03\"),\n",
    "    (\"D\", \"\",       200, \"2023-01-04\"),\n",
    "    (\"E\", \"Mumbai\", 200, None),\n",
    "]\n",
    "\n",
    "columns = [\"name\", \"city\", \"amount\", \"DOJ\"]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "print(\"Input Data:\")\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a453d730-f014-40a2-ad19-c6689485e270",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# ✅ Filter out rows where ANY column is NULL or blank\n",
    "filtered_df = df.filter(\n",
    "    (col(\"name\").isNotNull()) & (trim(col(\"name\")) != \"\") &\n",
    "    (col(\"city\").isNotNull()) & (trim(col(\"city\")) != \"\") &\n",
    "    (col(\"amount\").isNotNull()) &\n",
    "    (col(\"DOJ\").isNotNull()) & (trim(col(\"DOJ\")) != \"\")\n",
    ")\n",
    "\n",
    "print(\"Filtered Data:\")\n",
    "filtered_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b88abd39-46a4-4cf1-88ef-1945602f6c43",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from functools import reduce\n",
    "\n",
    "print(df.dtypes)\n",
    "print(dict(df.dtypes))\n",
    "conditions = []\n",
    "for c in df.columns:\n",
    "    # For string columns → check null & empty string\n",
    "    print(c)\n",
    "    #print(dict(df.dtypes)[c])\n",
    "    if dict(df.dtypes)[c] in (\"string\", \"varchar\", \"char\"):\n",
    "        conditions.append((col(c).isNotNull()) & (trim(col(c)) != \"\"))\n",
    "    else:\n",
    "        # For non-string columns → just check null\n",
    "        conditions.append(col(c).isNotNull())\n",
    "\n",
    "for i in conditions:\n",
    "    print(i)\n",
    "final_condition = reduce(lambda x, y: x & y, conditions)\n",
    "print(final_condition)\n",
    "\n",
    "# Apply filter\n",
    "filtered_df = df.filter(final_condition)\n",
    "\n",
    "print(\"Filtered Data:\")\n",
    "filtered_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "83a92962-22a9-4ef7-8ff5-468725207a69",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from functools import reduce\n",
    "\n",
    "conditions = []\n",
    "for c in df.columns:\n",
    "    # For string columns → check null & empty string\n",
    "    if dict(df.dtypes)[c] in (\"string\", \"varchar\", \"char\"):\n",
    "        conditions.append((col(c).isNotNull()) & (trim(col(c)) != \"\"))\n",
    "    else:\n",
    "        # For non-string columns → just check null\n",
    "        conditions.append(col(c).isNotNull())\n",
    "\n",
    "final_condition = reduce(lambda x, y: x & y, conditions)\n",
    "# Apply filter\n",
    "filtered_df = df.filter(final_condition)\n",
    "filtered_df.show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Filter Null column values",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
